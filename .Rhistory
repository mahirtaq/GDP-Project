"Angola",
"Papua New Guinea",
"Sao Tome and Principe",
"Comoros",
"Cape Verde",
"Mozambique",
"Grenada",
"Guinea-Bissau",
"Bahamas",
"Bahrain",
"United Arab Emirates",
"Qatar",
"Bangladesh",
"Fiji"
)
# Removing countries formed after 1970
quality_of_government = subset(quality_of_government, !(cname %in% countries_to_remove))
lower_year = 1980
upper_year = 2010
# Removing data before certain year
quality_of_government = subset(quality_of_government, year >= lower_year)
print("Dimension of data after removing countries and years")
dim(quality_of_government)
#list of surveys to keep
surveys_to_keep = c(
"^cname$",
"^year$",
"^aid_",
"^bci_",
"^fao_",
"^fi_",
"^gea_",
"^gle_",
"^gol_",
"^hf_",
"^ictd_",
"^icrg_",
"^mad_",
"^p_",
"^ciri_",
"^diat_",
"^dr_",
"^ef_",
"^eob_",
"^pwt_",
"^ross_",
"^shec_"
)
var_names_to_keep = c()
for (survey_code in surveys_to_keep){
indices = grep(survey_code, names(quality_of_government), value=TRUE)
var_names_to_keep = append(var_names_to_keep, indices)
}
quality_of_government = subset(quality_of_government, select=var_names_to_keep)
print("Dimension of dataset after choosing columns")
dim(quality_of_government)
# Create % change response variable
quality_of_government$gdp_percent_change = calculate_percent_changes(quality_of_government$mad_gdppc, quality_of_government$year)
# Deleting rows which do not have response variable
missing_gdp_row_id = which(is.na(quality_of_government$gdp_percent_change))
quality_of_government = quality_of_government[-missing_gdp_row_id,]
# Remove unneeded gdp predictors and
drops = c("gle_cgdpc", "gle_gdp", "gle_rgdpc", "pwt_rgdp", "pwt_slcgdp", "mad_gdppc", "X")
quality_of_government = quality_of_government[, !(names(quality_of_government) %in% drops)]
dim(quality_of_government)
#quality_of_government$year = quality_of_government$year-2018
#write.csv(quality_of_government, file="post_processing.csv")
# upper_year = 2010
# # Removing data after certain year
# quality_of_government = subset(quality_of_government, year <= upper_year)
nrows = nrow(quality_of_government)
ncols = ncol(quality_of_government)
# Plotting missing data by rows
num_na_row = rep(0, nrows)
for(i in 1:nrows){
num_na_row[i] = sum(is.na(quality_of_government[i,]))
}
percent_na_row = (num_na_row/ncols)*100
plot(seq(1,nrows),percent_na_row, xlab="rows", ylab="Percent of missing data")
# Plotting missing data by year
missing_by_year = rep(0, 2018)
for(i in 1:nrow(quality_of_government)){
missing_by_year[quality_of_government[i,]$year] =  missing_by_year[quality_of_government[i,]$year]+(percent_na_row[i])
}
plot(lower_year:upper_year, missing_by_year[lower_year:upper_year], xlab="Year", ylab="missing" )
# countries with more than x% missing data
# more_countries_to_remove = unique(quality_of_government$cname[which(percent_na_row >90)])
# quality_of_government = subset(quality_of_government, !(cname %in% more_countries_to_remove))
# print("Dimension after removing rows with more than 90 percent missing data:")
# dim(quality_of_government)
# Remove rows with more than 95 percent missing data
quality_of_government = quality_of_government[percent_na_row < 95,]
print("Dimension after removing rows with more than 95% missing data")
dim(quality_of_government)
# Plotting missing data by columns
nrows = nrow(quality_of_government)
num_na_col = rep(0, ncols)
for(i in 1:ncols){
num_na_col[i] = sum(is.na(quality_of_government[,i]))
}
percent_na_col = (num_na_col/nrows)*100
plot(seq(1,ncols),percent_na_col, xlab="cols", ylab="Percent of missing data")
# Plotting percentage of missing data vs number of rows with that much data missing
zero_to_hundred = seq(1,100)
rows_included = rep(0,100)
for(i in 1:100){
rows_included[i] = sum(percent_na_row < i)
}
plot(zero_to_hundred, rows_included, xlab="Percent of Missing Data", ylab="Num rows with less than that much missing data")
# Plotting percentage of missing data vs number of columns with that much data missing
zero_to_hundred = seq(1,100)
vars_included = rep(0,100)
for(i in 1:100){
vars_included[i] = sum(percent_na_col < i)
}
plot(zero_to_hundred, vars_included, xlab="Percent of Missing Data", ylab="Num variables with less than that much missing data")
#which(names(quality_of_government))
library('imputeTS')
country_names = unique(quality_of_government$cname)
mean_column = rep(1:ncol(quality_of_government))
#mean of the columns
for( i in 1 : ncol(quality_of_government)){
if(is.numeric(quality_of_government[,i]))
mean_column[i] <- mean(quality_of_government[,i], na.rm = TRUE)
}
#interpolation
for(country in country_names){
country_rows = which(quality_of_government$cname == country)
for(i in 1:ncol(quality_of_government)){
if(sum(!is.na(quality_of_government[country_rows, i])) >= 2 & is.numeric(quality_of_government[country_rows, i]))
quality_of_government[country_rows, i] = na.interpolation(quality_of_government[country_rows, i])
else if (!is.numeric(quality_of_government[country_rows, i]))
next
else
quality_of_government[country_rows, i] = mean_column[i]
}
}
# Removing data after certain year
quality_of_government = subset(quality_of_government, year <= upper_year)
# save dataset after entire preprocessing is done
save(quality_of_government, "quality-of-government-final.csv")
set.seed(420)
data = read.csv(file="data/quality-of-government-final.csv", header=TRUE, sep=",")
train_ids = sample(1:nrow(data), size = round(nrow(data)*0.8))
names(data)
train_data = data[train_ids, ]
test_data = data[-train_ids, ]
# Loading in data
source("setup.R")
set.seed(420)
data = read.csv(file="data/quality-of-government-final.csv", header=TRUE, sep=",")
train_ids = sample(1:nrow(data), size = round(nrow(data)*0.8))
train_data = data[train_ids, ]
test_data = data[-train_ids, ]
# Loading in data
library(glmnet)
# Creating matrices for lasso model
X_train = model.matrix(gdp_percent_change ~ . - cname, train_data)
y_train = train_data$mad_gdppc
# choosing best lambda by cv
cv.out = cv.glmnet(X_train, y_train, alpha = 1)
set.seed(420)
data = read.csv(file="data/quality-of-government-final.csv", header=TRUE, sep=",")
train_ids = sample(1:nrow(data), size = round(nrow(data)*0.8))
train_data = data[train_ids, ]
test_data = data[-train_ids, ]
# Loading in data
library(glmnet)
# Creating matrices for lasso model
X_train = model.matrix(gdp_percent_change ~ . - cname, train_data)
y_train = train_data$gdp_percent_change
# choosing best lambda by cv
cv.out = cv.glmnet(X_train, y_train, alpha = 1)
bestlam = cv.out$lambda.min
# Fitting model, might have to set grid of lambdas manually
lasso.mod = glmnet(X_train, y_train, alpha=1)
#lasso.pred_test = predict(lasso.mod, s=bestlam, newx=X_test)
lasso.coef = predict(lasso.mod, type="coefficients", s=bestlam)
lasso.coef
plot(lasso.mod, xvar = "lambda", label= TRUE)
set.seed(420)
data = read.csv(file="data/quality-of-government-final.csv", header=TRUE, sep=",")
train_ids = sample(1:nrow(data), size = round(nrow(data)*0.8))
train_data = data[train_ids, ]
test_data = data[-train_ids, ]
# Loading in data
library(glmnet)
# Creating matrices for lasso model
X_train = model.matrix(gdp_percent_change ~ . - cname, train_data)
y_train = train_data$gdp_percent_change
X_test = model.matrix(gdp_percent_change ~ . - cname, test_data)
y_test = test_data$gdp_percent_change
# choosing best lambda by cv
cv.out = cv.glmnet(X_train, y_train, alpha = 1)
bestlam = cv.out$lambda.min
# Fitting model, might have to set grid of lambdas manually
lasso.mod = glmnet(X_train, y_train, alpha=1)
#lasso.pred_test = predict(lasso.mod, s=bestlam, newx=X_test)
lasso.coef = predict(lasso.mod, type="coefficients", s=bestlam)
lasso.coef
plot(lasso.mod, xvar = "lambda", label= TRUE)
lasso.pred_test = predict(lasso.mod, s=bestlam, newx=X[test,])
lasso.pred_test = predict(lasso.mod, s=bestlam, newx=X_test)
mean((lasso.pred_test - y_test)^2)
set.seed(420)
data = read.csv(file="data/quality-of-government-final.csv", header=TRUE, sep=",")
train_ids = sample(1:nrow(data), size = round(nrow(data)*0.8))
train_data = data[train_ids, ]
test_data = data[-train_ids, ]
# Loading in data
library(glmnet)
# Creating matrices for lasso model
X_train = model.matrix(gdp_percent_change ~ . - cname, train_data)
y_train = train_data$gdp_percent_change
X_test = model.matrix(gdp_percent_change ~ . - cname, test_data)
y_test = test_data$gdp_percent_change
# choosing best lambda by cv
cv.out = cv.glmnet(X_train, y_train, alpha = 1)
bestlam = cv.out$lambda.min
# Fitting model, might have to set grid of lambdas manually
lasso.mod = glmnet(X_train, y_train, alpha=1)
#lasso.pred_test = predict(lasso.mod, s=bestlam, newx=X_test)
lasso.coef = predict(lasso.mod, type="coefficients", s=bestlam)
lasso.coef
plot(lasso.mod, xvar = "lambda", label= TRUE)
lasso.pred_test = predict(lasso.mod, s=bestlam, newx=X_test)
lasso.pred_train = predict(lasso.mod, s=bestlam, newx=X_train)
mean((lasso.pred_test - y_test)^2)
mean((lasso.pred_train - y_train)^2)
?USArrests
head(USArrests0
head(USArrests)
head(USArrests)
hc.complete <- hclust(dist(USArrests), method = "complete")
hc.complete <- hclust(dist(USArrests), method = "complete")
plot(hc.complete, main = "Complete Linkage", xlab = "", sub = "", cex = 9)
names(USArrests)
hc.complete <- hclust(dist(USArrests$Murder), method = "complete")
plot(hc.complete, main = "Complete Linkage", xlab = "", sub = "", cex = 9)
x=matrix(rnorm(50*2), ncol=2)
x[1:25,1] = x[1:25,1]+3
x[1:25,2] = x[1:25,2]-4
hc.complete =hclust(dist(x), method="complete")
plot(hc.complete,main="Complete Linkage", xlab="", sub="", cex=.9)
hc.complete <- hclust(dist(USArrests), method = "complete")
plot(hc.complete, main = "Complete Linkage", xlab = "", sub = "", cex = .9)
hc.complete <- hclust(dist(USArrests$Murder), method = "complete")
plot(hc.complete, main = "Complete Linkage", xlab = "", sub = "", cex = .9)
hc.complete <- hclust(dist(USArrests), method = "complete")
plot(hc.complete, main = "Complete Linkage", xlab = "States", sub = "", cex = .9)
c5.complete = cutree(hc.complete, 5)
c5.complete
install.packages("cluster")
library(cluster)
sil.complete <- sillhouett(c5.complete, dist = dist(USArrests))
library(cluster)
sil.complete <- sillhouette(c5.complete, dist = dist(USArrests))
library(cluster)
sil.complete <- silhouette(c5.complete, dist = dist(USArrests))
plot(sil.complete, main = "Complete Linkage")
hc.single <- hclust(dist(USArrests), method = "single")
plot(hc.complete, main = "Single Linkage", xlab = "States", sub = "", cex = .9)
hc.single <- hclust(dist(USArrests), method = "single")
plot(hc.single, main = "Single Linkage", xlab = "States", sub = "", cex = .9)
c5.single = cutree(hc.single, 5)
c5.single
km.out <- kmeans(USArrests, 5, nstart = 20)
km.out$tot.withinss
km.out <- kmeans(USArrests, 5, nstart = 50)
km.out$tot.withinss
km.out <- kmeans(USArrests, 5, nstart = 20)
km.out$tot.withinss
km.out <- kmeans(USArrests, 5, nstart = 1)
km.out$tot.withinss
km.out <- kmeans(USArrests, 5, nstart = 20)
km.out$tot.withinss
hc.complete <- hclust(dist(USArrests), method = "complete")
plot(hc.complete, main = "Complete Linkage", xlab = "States", sub = "", cex = .9)
c5.complete <- cutree(hc.complete, 5)
c5.complete
km.out
km.out$cluster
sil.km <- silhouette(km.out$cluster, dist = dist(USArrests))
library(cluster)
sil.complete <- silhouette(c5.complete, dist = dist(USArrests))
plot(sil.complete, main = "Complete Linkage")
sil.km <- silhouette(km.out$cluster, dist = dist(USArrests))
plot(sil.km, main = "K Means Silhouette")
set.seed(420)
data = read.csv(file="data/quality-of-government-final.csv", header=TRUE, sep=",")
train_ids = sample(1:nrow(data), size = round(nrow(data)*0.8))
train_data = data[train_ids, ]
test_data = data[-train_ids, ]
names(train_data)
# Loading in data
set.seed(420)
data = read.csv(file="data/quality-of-government-final.csv", header=TRUE, sep=",")
train_ids = sample(1:nrow(data), size = round(nrow(data)*0.8))
train_data = data[train_ids, ]
test_data = data[-train_ids, ]
library(glmnet)
# Creating matrices for lasso model
X_train = model.matrix(gdp_percent_change ~ . - cname - X, train_data)
y_train = train_data$gdp_percent_change
X_test = model.matrix(gdp_percent_change ~ . - cname - X, test_data)
y_test = test_data$gdp_percent_change
# choosing best lambda by cv
cv.out = cv.glmnet(X_train, y_train, alpha = 1)
bestlam = cv.out$lambda.min
# Fitting model, might have to set grid of lambdas manually
lasso.mod = glmnet(X_train, y_train, alpha=1)
lasso.coef = predict(lasso.mod, type="coefficients", s=bestlam)
lasso.coef
plot(lasso.mod, xvar = "lambda", label= TRUE)
lasso.pred_test = predict(lasso.mod, s=bestlam, newx=X_test)
lasso.pred_train = predict(lasso.mod, s=bestlam, newx=X_train)
mean((lasso.pred_test - y_test)^2)
mean((lasso.pred_train - y_train)^2)
?saveRDS
lasso.pred_test = predict(lasso.mod, s=bestlam, newx=X_test)
lasso.pred_train = predict(lasso.mod, s=bestlam, newx=X_train)
mean((lasso.pred_test - y_test)^2)
mean((lasso.pred_train - y_train)^2)
saveRDS(lasso.mod, file = "lasso-model.rds")
saveRDS(lasso.pred_test, file = "lasso-prediction.rds")
View(data)
# choosing best lambda by cv
cv.out = cv.glmnet(X_train, y_train, alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
# choosing best lambda by cv
cv.out = cv.glmnet(X_train, y_train, alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
# Fitting model, might have to set grid of lambdas manually
lasso.mod = glmnet(X_train, y_train, alpha=1)
lasso.coef = predict(lasso.mod, type="coefficients", s=bestlam)
lasso.coef
# choosing best lambda by cv
cv.out = cv.glmnet(X_train, y_train, alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
names(cv.out)
?cv.out
# choosing best lambda by cv
cv.out = cv.glmnet(X_train, y_train, alpha = 1)
set.seed(420)
data = read.csv(file="data/quality-of-government-final.csv", header=TRUE, sep=",")
train_ids = sample(1:nrow(data), size = round(nrow(data)*0.8))
train_data = data[train_ids, ]
test_data = data[-train_ids, ]
library(glmnet)
# Creating matrices for lasso model
X_train = model.matrix(gdp_percent_change ~ . - cname - X, train_data)
y_train = train_data$gdp_percent_change
X_test = model.matrix(gdp_percent_change ~ . - cname - X, test_data)
y_test = test_data$gdp_percent_change
# choosing best lambda by cv
cv.out = cv.glmnet(X_train, y_train, alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
cv_error = cv.out$cvm
cv_error
bestlam = cv.out$lambda.min
cv_error = cv.out$cvm
cv_error.min
bestlam = cv.out$lambda.min
cv_error = cv.out$cvm
min(cv_error)
bestlam = cv.out$lambda.min
cv_error = cv.out$cvm
cv_error
bestlam = cv.out$lambda.min
cv_error = cv.out$cvm
min(cv_error)
# choosing best lambda by cv and plotting
cv.out = cv.glmnet(X_train, y_train, alpha = 1)
plot(cv.out)
# choosing best lambda by cv and plotting
cv.out = cv.glmnet(X_train, y_train, alpha = 1)
plot(cv.out, main = "CV Error vs Log Lambda")
# Finding best lambda and reporting its cv error
bestlam = cv.out$lambda.min
cv_error = min(cv.out$cvm)
bestlam
cv_error
# Finding best lambda and reporting its cv error
bestlam = cv.out$lambda.min
cv_error = min(cv.out$cvm)
bestlam
cv_error
# Finding best lambda and reporting its cv error
bestlam = cv.out$lambda.min
cv_error = min(cv.out$cvm)
bestlam
cv_error
set.seed(420)
data = read.csv(file="data/quality-of-government-final.csv", header=TRUE, sep=",")
train_ids = sample(1:nrow(data), size = round(nrow(data)*0.8))
train_data = data[train_ids, ]
test_data = data[-train_ids, ]
library(glmnet)
# Creating matrices for lasso model
X_train = model.matrix(gdp_percent_change ~ . - cname - X, train_data)
y_train = train_data$gdp_percent_change
X_test = model.matrix(gdp_percent_change ~ . - cname - X, test_data)
y_test = test_data$gdp_percent_change
# choosing best lambda by cv and plotting
cv.out = cv.glmnet(X_train, y_train, alpha = 1)
plot(cv.out, main = "CV Error vs Log Lambda")
# Finding best lambda and reporting its cv error
bestlam = cv.out$lambda.min
cv_error = min(cv.out$cvm)
bestlam
cv_error
# Finding best lambda and reporting its cv error
bestlam = cv.out$lambda.min
cv_error = min(cv.out$cvm)
bestlam
cv_error
# Finding best lambda and reporting its cv error
bestlam = cv.out$lambda.min
cv_error = min(cv.out$cvm)
bestlam
cv_error
set.seed(420)
data = read.csv(file="data/quality-of-government-final.csv", header=TRUE, sep=",")
train_ids = sample(1:nrow(data), size = round(nrow(data)*0.8))
train_data = data[train_ids, ]
test_data = data[-train_ids, ]
library(glmnet)
# Creating matrices for lasso model
X_train = model.matrix(gdp_percent_change ~ . - cname - X, train_data)
y_train = train_data$gdp_percent_change
X_test = model.matrix(gdp_percent_change ~ . - cname - X, test_data)
y_test = test_data$gdp_percent_change
# choosing best lambda by cv and plotting
cv.out = cv.glmnet(X_train, y_train, alpha = 1)
plot(cv.out, main = "CV Error vs Log Lambda")
# Finding best lambda and reporting its cv error
bestlam = cv.out$lambda.min
cv_error = min(cv.out$cvm)
bestlam
cv_error
# Fitting model, might have to set grid of lambdas manually
lasso.mod = glmnet(X_train, y_train, alpha=1)
lasso.coef = predict(lasso.mod, type="coefficients", s=bestlam)
lasso.coef
plot(lasso.mod, xvar = "lambda", label= TRUE)
lasso.pred_test = predict(lasso.mod, s=bestlam, newx=X_test)
lasso.pred_train = predict(lasso.mod, s=bestlam, newx=X_train)
mean((lasso.pred_test - y_test)^2)
mean((lasso.pred_train - y_train)^2)
saveRDS(lasso.mod, file = "lasso-model.rds")
saveRDS(lasso.pred_test, file = "lasso-prediction.rds")
# Finding best lambda and reporting its cv error
bestlam = cv.out$lambda.min
cv_error = min(cv.out$cvm)
selam = cv.out$lambda.1se
bestlam
cv_error
# Fitting model, might have to set grid of lambdas manually
lasso.mod = glmnet(X_train, y_train, alpha=1)
lasso.coef = predict(lasso.mod, type="coefficients", s=selam)
lasso.coef
# Finding best lambda and reporting its cv error
bestlam = cv.out$lambda.min
cv_error = min(cv.out$cvm)
selam = cv.out$lambda.1se
selam
bestlam
cv_error
# Fitting model, might have to set grid of lambdas manually
lasso.mod = glmnet(X_train, y_train, alpha=1)
lasso.coef = predict(lasso.mod, type="coefficients", s=selam - 1)
lasso.coef
# Finding best lambda and reporting its cv error
bestlam = cv.out$lambda.min
cv_error = min(cv.out$cvm)
selam = cv.out$lambda.1se
selam = selam - 1 # subtract 1 since one stderr drops all predictors
selam
bestlam
cv_error
knitr::opts_chunk$set(echo = TRUE)
source("setup.r")
getwd()
setwd("C:/Users/mahir/Desktop/stats-415-project/")
source("setup.r")
source("setup.R")
library(boot)
# Linear Regresssion
glm.fit =glm(gdp_percent_change~., data=train_data)
cv.error= cv.glm(train_data, glm.fit, K=10)$delta[1]
linear_regression_model = lm(gdp_percent_change ~ ., data = train_data)
train_predictions = predict.lm(
linear_regression_model, train_data)
test_predictions = predict.lm(
linear_regression_model, test_data)
train_error_mse = mean(
(train_predictions - train_data$gdp_percent_change)^2)
test_error_mse = mean(
(test_predictions - test_data$gdp_percent_change)^2)
summary(linear_regression_model)
cv.error
train_error_mse
test_error_mse
summary(linear_regression_model)
summary(linear_regression_model)$coeff
summary(linear_regression_model)$coeff[-1, 4]
summary(linear_regression_model)$coeff[-1, 4] < 0.01
summary(linear_regression_model)
